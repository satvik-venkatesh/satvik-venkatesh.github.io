<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Satvik Venkatesh</title>
	<link rel="stylesheet" href="bootstrap-4.1.3-dist/css/bootstrap.min.css">
	<link rel="stylesheet" href="style-blog.css">
	<link rel="stylesheet" href="css/fixed.css">
</head>

<body data-spy="scroll" data-target="#navbarResponsive">


<!--- Navigation -->
<nav class="navbar navbar-expand-md navbar-dark bg-dark fixed-top">
	<a class="navbar-brand" href="index.html#home"><img src="img/piano3.png"></a>
	<button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive">
		<span class="navbar-toggler-icon"></span>
	</button>

	<div class="collapse navbar-collapse" id="navbarResponsive">
		<ul class="navbar-nav ml-auto">
			<li class="nav-item">
				<a class="nav-link" href="index.html#home">Home</a>				
			</li>
			<li class="nav-item">
				<a class="nav-link" href="index.html#publications">Publications</a>				
			</li>
			<li class="nav-item">
				<a class="nav-link" href="index.html#music">Music</a>				
			</li>
			<li class="nav-item">
				<a class="nav-link" href="index.html#about">About me</a>				
			</li>			
			<li class="nav-item">
				<a class="nav-link" href="#blog-post">Blog</a>
		</ul>
	</div>
</nav>

<!--- End Navigation -->


<!--- Start Blog Section -->
<div id="blog-post">

<!--- Start Jumbotron -->
<div class="jumbotron text-center">
	<div class="narrow">

		<div class="col-12">
			<h2 class="heading-blog">Data Generators with Keras and Tensorflow on Google Colab</h2>
			<div class="heading-underline"></div>			
		</div>

		<div class="row text-justify">

			<div class="col-md-12">
				<div class="blog-body">
					<h4>1. &nbsp; Introduction</h4>
					<p>This blog post is a tutorial on using data generators with Keras on Google Colab. Data generators allow you to feed data into Keras in real-time while training the model. This way, you can make modifications to the data before feeding it to the neural network or even load it from the secondary memory. Data generators have two use cases â€” (1) Data augmentation and (2) loading a dataset that does not fit into the RAM.</p>

					<p>There are many posts out there that explain the use of data generators. Most of them explain in the context of using a local computer. Recently, many people have started using Google Colab for machine learning projects. Using data generators with Google Colab was trickier than I expected. For example, the delay while directly loading files from Google drive. Therefore, this post explains some of the dos and don'ts while using data generators with Google Colab. My research is focused on audio classification. So, some of my opinions might be biased towards an audio-context. Below is the definition of a data generator.</p>

					<div class="code-block text-left">
					<pre>
						<code>
							<p>import&nbsp;tensorflow&nbsp;as&nbsp;tf<br>import&nbsp;keras<br><br>class&nbsp;DataGenerator(tf.compat.v2.keras.utils.Sequence):<br>&nbsp;&nbsp;def&nbsp;__init__(self,&nbsp;list_examples,&nbsp;batch_size=64,&nbsp;dim=(802,&nbsp;80),<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;n_classes=2,&nbsp;shuffle=True):<br>&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Constructor&nbsp;of&nbsp;the&nbsp;data&nbsp;generator.<br>&nbsp;&nbsp;&nbsp;&nbsp;self.dim&nbsp;=&nbsp;dim<br>&nbsp;&nbsp;&nbsp;&nbsp;self.batch_size&nbsp;=&nbsp;batch_size<br>&nbsp;&nbsp;&nbsp;&nbsp;self.epoch_size&nbsp;=&nbsp;epoch_size<br>&nbsp;&nbsp;&nbsp;&nbsp;self.list_examples&nbsp;=&nbsp;list_examples<br>&nbsp;&nbsp;&nbsp;&nbsp;self.n_classes&nbsp;=&nbsp;n_classes<br>&nbsp;&nbsp;&nbsp;&nbsp;self.shuffle&nbsp;=&nbsp;shuffle<br>&nbsp;&nbsp;&nbsp;&nbsp;self.on_epoch_end()<br><br>&nbsp;&nbsp;def&nbsp;__len__(self):<br>&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Denotes&nbsp;the&nbsp;number&nbsp;of&nbsp;batches&nbsp;per&nbsp;epoch<br>&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;int(np.floor(len(self.list_examples)&nbsp;/&nbsp;self.batch_size))<br><br>&nbsp;&nbsp;def&nbsp;__getitem__(self,&nbsp;index):<br>&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Generate&nbsp;one&nbsp;batch&nbsp;of&nbsp;data<br>&nbsp;&nbsp;&nbsp;&nbsp;indexes&nbsp;=&nbsp;self.indexes[index*self.batch_size:(index+1)*self.batch_size]<br><br>&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Find&nbsp;list&nbsp;of&nbsp;IDs<br>&nbsp;&nbsp;&nbsp;&nbsp;list_IDs_temp&nbsp;=&nbsp;[self.list_examples[k]&nbsp;for&nbsp;k&nbsp;in&nbsp;indexes]<br><br>&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Generate&nbsp;data<br>&nbsp;&nbsp;&nbsp;&nbsp;X,&nbsp;y&nbsp;=&nbsp;self.__data_generation(list_IDs_temp)<br><br>&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;X,&nbsp;y<br><br>&nbsp;&nbsp;def&nbsp;on_epoch_end(self):<br>&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;This&nbsp;function&nbsp;is&nbsp;called&nbsp;at&nbsp;the&nbsp;end&nbsp;of&nbsp;each&nbsp;epoch.<br>&nbsp;&nbsp;&nbsp;&nbsp;self.indexes&nbsp;=&nbsp;np.arange(len(self.list_examples))<br>&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;self.shuffle&nbsp;==&nbsp;True:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;np.random.shuffle(self.indexes)<br><br>&nbsp;&nbsp;def&nbsp;__data_generation(self,&nbsp;list_IDs_temp):<br>&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Load&nbsp;individual&nbsp;numpy&nbsp;arrays&nbsp;and&nbsp;aggregate&nbsp;them&nbsp;to&nbsp;a&nbsp;batch.<br>&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;X&nbsp;=&nbsp;np.empty([self.batch_size,&nbsp;dim[0],&nbsp;dim[1]],&nbsp;dtype=np.float32)<br>&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;`y`&nbsp;is&nbsp;a&nbsp;one-hot&nbsp;encoded&nbsp;vector.<br>&nbsp;&nbsp;&nbsp;&nbsp;y&nbsp;=&nbsp;np.empty([self.batch_size,&nbsp;dim[0],&nbsp;n_classes],&nbsp;dtype=np.int16)<br><br>&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Generate&nbsp;data.<br>&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;i,&nbsp;ID&nbsp;in&nbsp;enumerate(list_IDs_temp):<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Load&nbsp;sample<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;X[i,:,&nbsp;:]&nbsp;=&nbsp;np.load(ID[0])<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Load&nbsp;labels&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;y[i,&nbsp;:,&nbsp;:]&nbsp;=&nbsp;np.load(ID[1])<br><br>&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;X,&nbsp;y</p>
						</code>
					</pre>
					</div>

					<h4>2. &nbsp; Save individual examples as NumPy arrays</h4>
					<p>Format each example in the dataset as a separate NumPy array. Store the train, validation, and test data into separate directories. If you have thousands of examples, the 'glob' module is not good at loading the files. Hence, it is a good idea to divide your dataset into sub-folders or blocks. For example, '/content/train data/block-id-1/id-1.npy', ... , '/content/train data/block-id-3/id-642.npy', etc. I generally store 320  examples per block.</p>

					<p>You can store the labels and training as NumPy files within the same folder. For examples, '/content/train data/block-id-1/id-label-1.npy'.</p>

					<h4>3. &nbsp; Save the database as a zipped file</h4>
					<p> As per my experience, the best way to store data in Google drive is as a .zip file. When you want to import the database into the Colab notebook, there are two ways of doing it. (1) Extract the zip file directly from the drive, as shown in [figure], or (2) use a !wget command to download it to the notebook and subsequently extract it. In order to do this, you will have to create a shareable link of the zip file in drive. The first method is most convenient and works fine in most cases. However, if your zip file is >10 GB and you extract it directly from the drive multiple times, then your Google Drive API begins to fail. There are some posts in stack overflow that speak about the Read/Write limits imposed by the Google Drive API. However, this is a problem that I have personally encountered as well. Hence, if you have too much data, think about using the !wget command.</p> 

					<p>It is convenient to store the train, validation, and test sets as separate .zip files. This way you can easily extract them into separate directories in the virtual machine.</p>

					<h4>4. &nbsp; Do not load the dataset directly from Google drive</h4>
					<p>I learnt this the hard way. It might be tempting to store your dataset in Google Drive and load examples directly from it. In Google Colab, files are loaded from drive through the Google Drive API. The files are not stored on the virtual machine. Therefore, loading files for the data generator directly from Google Drive would lead to a major bottleneck while training your neural network.</p>

					<h4>5. &nbsp; Some Failed Approaches</h4>
					<p>Initially, when I was trying to load the NumPy arrays directly from Google Drive. A hack that I came up with was to load batches instead of individual files. This would reduce the number of files that Google Drive API has to load. For example, if the batch size was 128, each NumPy file would contain 128 examples. The problem with this approach is that there is no straightforward way to shuffle the data while training.</p>
				</div>
			</div>

		</div> <!--- End Row -->
	</div> <!--- End Narrow -->
</div> <!--- End Jumbotron -->

</div>
<!--- End Blog Section -->




<!--- Script Source Files -->
<script src="js/jquery-3.3.1.min.js"></script>
<script src="bootstrap-4.1.3-dist/js/bootstrap.min.js"></script>
<script src="https://use.fontawesome.com/releases/v5.5.0/js/all.js"></script>
<!--- End of Script Source Files -->

</body>
</html>